---
categories:
- ""
- ""
date: "2017-10-31T22:42:51-05:00"
description: Nullam et orci eu lorem consequat tincidunt vivamus et sagittis magna
  sed nunc rhoncus condimentum sem. In efficitur ligula tate urna. Maecenas massa
  sed magna lacinia magna pellentesque lorem ipsum dolor. Nullam et orci eu lorem
  consequat tincidunt. Vivamus et sagittis tempus.
draft: false
image: Romney.jpeg
keywords: ""
slug: hamish
title: Hamish Thomas - Pre-course Assignment
---

```{r load-libraries, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(gapminder)  # gapminder dataset
library(here)
library(janitor)
```


# Task 1: Short Auto-Biography
## By Hamish Thomas

My name is *Hamish* *Thomas* and I was born on the 11th November 1998 in south-west London. When I was just 5 years old, I moved to America for family reasons, and as a result I spent a large percentage of my childhood living in Los Angeles, California. I later returned to the UK where I completed my GCSE and IB qualifications, which led me to an **undergraduate** **engineering** **degree** at [Imperial College London](https://www.imperial.ac.uk/), in which I achieved a First-Class Honours and an award for my dissertation on COVID-19 particle dispersion. 

My previous professional experiences vary from the engineering sector, where I worked with London's Crossrail project and a large automotive consultancy in Berlin, to digital advertising and even the National Health Service (NHS). My primary personal interests are as follows: 

* Politics
* The environment 
* Running 
* Guitar 
* Football (which I like to coach, play and watch - primarily as an over-optimistic supporter of Tottenham Hotspur). 

![A photo I took from the North Stand at the Tottenham Hotspur Stadium in 2019 after a 1-0 win against Brighton in which Christian Eriksen scored the only goal.](SpursStadium.JPG)




# Task 2: `gapminder` country comparison

You have seen the `gapminder` dataset that has data on life expectancy, population, and GDP per capita for 142 countries from 1952 to 2007. Below is a 'glimpse' of the data frame, as well as the first 20 lines within the 'table'. 


```{r}
glimpse(gapminder)
head(gapminder, 20) # look at the first 20 rows of the dataframe
```



Your task is to produce two graphs of how life expectancy has changed over the years for the `country` and the `continent` you come from.

We will now produce three graphs which show how the life expectancy for different regions evolves over time. 

```{r}
country_data <- gapminder %>% 
            filter(country == "United Kingdom") 
continent_data <- gapminder %>% 
            filter(continent == "Europe")
```



## Life Expectancy in the United Kingdom over time. 

First, we plot of life expectancy over time for a single country of choice - the *United Kingdom*. This is done by mapping `year` on the x-axis, and `lifeExp` on the y-axis.

```{r, lifeExp_one_country}
plot1 <- ggplot(data = country_data, mapping = aes(x = year, y = lifeExp))+
   geom_point() +
   geom_smooth(se = FALSE)+
   NULL 
```

```{r, lifeExp_one_country_with_label}
plot1<- plot1 +
   labs(title = "Average UK Life Expectancy over time.",
       x = "Year",
       y = "Average Life Expectancy") +
   NULL

plot1
```


## Life Expectancy in Europe over time.

Secondly, we produce a plot for all countries in *Europe*. This is done by mapping the `country` variable to the colour aesthetic. We also map `country` to the `group` aesthetic, so that all points for each country are grouped together.

```{r lifeExp_one_continent}
ggplot(continent_data, mapping = aes(x = year  , y = lifeExp , colour = country , group = country))+
  geom_line() + 
  geom_smooth(se = FALSE) +
  labs(title = "Average Life Expectancy over time for European Countries.",
       x = "Year",
       y = "Average Life Expectancy") +
  NULL
```



## Life Expectancy in different continents over time.

Finally, using the original `gapminder` data, we produce a life expectancy over time graph, grouped (or faceted) by continent. The legend is removed by adding the `theme(legend.position="none")` in the end of our ggplot.

```{r lifeExp_facet_by_continent}
ggplot(data = gapminder , mapping = aes(x = year , y = lifeExp  , colour = continent ))+
  geom_point() + 
  geom_smooth(se = FALSE) +
  facet_wrap(~continent) +
  theme(legend.position="none") + #remove all legends
  labs(title = "Average Life Expectancy over time for different Continents.",
       x = "Year",
       y = "Average Life Expectancy") +
  NULL
```

### Given these trends, what can you say about life expectancy since 1952? 

The continents that have seen the sharpest rises in life expectancy since 1952 are those which contain emerging economies. Countries that have exhibited substantial economic growth in this period, such as Brazil, China, India, Indonesia, Malaysia and Mexico, are typically located in Asia or the Americas. This is likely why these continents seem to have exhibited the highest growth, as the economic development within their countries has led to improved healthcare, technology, diet and living standards. 

Growth in Africa followed an upward trend between 1950 - 1990 that exhibited a similar gradient to other fast-growing continents. But this increaase levelled off shortly after, which is likely due to limited economic power (which is enhanced by major corruption in many of these countries), poor healthcare as well as many conflicts/wars on this continent leading to many young deaths. 

As expected, continents whose economic growth primarily accelerated before the 1950s - namely Europe and Oceania - exhibit a smaller increase in life expectancy during the above time-frame. 







> Type your answer after this blockquote.

# Task 3: Brexit vote analysis

We will have a look at the results of the 2016 Brexit vote in the UK. First we read the data using `read_csv()` and have a quick glimpse at the data (shown below).

```{r load_brexit_data, warning=FALSE, message=FALSE}
brexit_results <- read_csv(here::here("data","brexit_results.csv"))
glimpse(brexit_results)
```

The data comes from [Elliott Morris](https://www.thecrosstab.com/), who cleaned it and made it available through his [DataCamp class on analysing election and polling data in R](https://www.datacamp.com/courses/analyzing-election-and-polling-data-in-r).

Our main outcome variable (or y) is `leave_share`, which is the percent of votes cast in favour of Brexit, or leaving the EU. Each row is a UK [parliament constituency](https://en.wikipedia.org/wiki/United_Kingdom_Parliament_constituencies).

To get a sense of the spread, or distribution, of the data, we can plot a *histogram*, a *density* *plot*, and the empirical *cumulative* *distribution* *function* of the leave % in all constituencies.

```{r brexit_histogram, warning=FALSE, message=FALSE}

# histogram
ggplot(brexit_results, aes(x = leave_share)) +
  geom_histogram(binwidth = 2.5) +
  labs(title = "Brexit Leave Share - Histogram",
       x = "Leave Share [%]",
       y = "Frequency of Constituencies",
       subtitle = "Figure 3.1")

# density plot-- think smoothed histogram
ggplot(brexit_results, aes(x = leave_share)) +
  geom_density()+
  labs(title = "Brexit Leave Share - Density Plot",
       x = "Leave Share [%]",
       y = "Frequency of Constituencies",
       subtitle = "Figure 3.2")


# The empirical cumulative distribution function (ECDF) 
ggplot(brexit_results, aes(x = leave_share)) +
  stat_ecdf(geom = "step", pad = FALSE) +
  scale_y_continuous(labels = scales::percent)+
  labs(title = "Brexit Leave Share - Cumulative Distribution Function Plot",
       x = "Leave Share [%]",
       y = "Frequency of Constituencies",
       subtitle = "Figure 3.3")
  


```


One common explanation for the Brexit outcome was fear of immigration and opposition to the EU's more open border policy. We can check the relationship (or correlation) between the proportion of native born residents (`born_in_uk`) in a constituency and its `leave_share`. To do this, let us get the correlation between the two variables

```{r brexit_immigration_correlation}
brexit_results %>% 
  select(leave_share, born_in_uk) %>% 
  cor()
```


The correlation is almost 0.5, which shows that the two variables are positively correlated.

We can also create a scatterplot between these two variables using `geom_point`. We also add the best fit line, using `geom_smooth(method = "lm")`.

```{r brexit_immigration_plot}
ggplot(brexit_results, aes(x = born_in_uk, y = leave_share)) +
  geom_point(alpha=0.3) +
  
  # add a smoothing line, and use method="lm" to get the best straight-line
  geom_smooth(method = "lm") + 
  
  # use a white background and frame the plot with a black box
  theme_bw() +
  labs(title = "Relationship between the proportion of native born residents and leave share.",
       x = "Born in the UK [%]",
       y = "Leave Share [%]",
       subtitle = "Figure 3.4")
  NULL
```


### What can you say about the relationship shown above? 

This relationship clearly shows that constituencies with a larger percentage of native British people are more likely to have a larger leave share. This is expected as those born in the UK are expected to feel more nationalistic when it comes to the key issues of Brexit - namely, immigration, burden on the NHS and other public services, etc. 




# Task 4: Animal rescue incidents attended by the London Fire Brigade

[The London Fire Brigade](https://data.london.gov.uk/dataset/animal-rescue-incidents-attended-by-lfb) attends a range of non-fire incidents (which we call 'special services'). These 'special services' include assistance to animals that may be trapped or in distress. The data is provided from January 2009 and is updated monthly. A range of information is supplied for each incident including some location information (postcode, borough, ward), as well as the data/time of the incidents. We do not routinely record data about animal deaths or injuries.

Please note that any cost included is a notional cost calculated based on the length of time rounded up to the nearest hour spent by Pump, Aerial and FRU appliances at the incident and charged at the current Brigade hourly rate. A 'glimpse' of the dataset is shown below: 

```{r load_animal_rescue_data, warning=FALSE, message=FALSE}

url <- "https://data.london.gov.uk/download/animal-rescue-incidents-attended-by-lfb/8a7d91c2-9aec-4bde-937a-3998f4717cd8/Animal%20Rescue%20incidents%20attended%20by%20LFB%20from%20Jan%202009.csv"

animal_rescue <- read_csv(url,
                          locale = locale(encoding = "CP1252")) %>% 
  janitor::clean_names()


glimpse(animal_rescue)
```

One of the more useful things one can do with any data set is quick counts, namely to see how many observations fall within one category. For instance, if we wanted to count the number of incidents by year, we would either use `group_by()... summarise()` or, simply [`count()`](https://dplyr.tidyverse.org/reference/count.html)

```{r, instances_by_calendar_year}

animal_rescue %>% 
  dplyr::group_by(cal_year) %>% 
  summarise(count=n())

animal_rescue %>% 
  count(cal_year, name="count")

```


Let us try to see how many incidents we have by animal group. Again, we can do this either using group_by() and summarise(), or by using count()...

```{r, animal_group_percentages}
animal_rescue %>% 
  group_by(animal_group_parent) %>% 
  
  #group_by and summarise will produce a new column with the count in each animal group
  summarise(count = n()) %>% 
  
  # mutate adds a new column; here we calculate the percentage
  mutate(percent = round(100*count/sum(count),2)) %>% 
  
  # arrange() sorts the data by percent. Since the default sorting is min to max and we would like to see it sorted
  # in descending order (max to min), we use arrange(desc()) 
  arrange(desc(percent))


animal_rescue %>% 
  
  #count does the same thing as group_by and summarise
  # name = "count" will call the column with the counts "count" ( exciting, I know)
  # and 'sort=TRUE' will sort them from max to min
  count(animal_group_parent, name="count", sort=TRUE) %>% 
  mutate(percent = round(100*count/sum(count),2))


```



### Do you see anything strange in these tables? 

Cat is reported twice in this table - once at number 1. - 'Cat' and again at number 11. - 'cat'. This highlights the case sensitivity of our categorisation. In reality, we need to combine these categories. 


Finally, let us have a loot at the notional cost for rescuing each of these animals. As the LFB says,

> Please note that any cost included is a notional cost calculated based on the length of time rounded up to the nearest hour spent by Pump, Aerial and FRU appliances at the incident and charged at the current Brigade hourly rate.

There is two things we will do:

1. Calculate the mean and median `incident_notional_cost` for each `animal_group_parent`
2. Plot a boxplot to get a feel for the distribution of `incident_notional_cost` by `animal_group_parent`.


Before we go on, however, we need to fix `incident_notional_cost` as it is stored as a `chr`, or character, rather than a number.

```{r, parse_incident_cost,message=FALSE, warning=FALSE}

# what type is variable incident_notional_cost from dataframe `animal_rescue`
typeof(animal_rescue$incident_notional_cost)

# readr::parse_number() will convert any numerical values stored as characters into numbers
animal_rescue <- animal_rescue %>% 

  # we use mutate() to use the parse_number() function and overwrite the same variable
  mutate(incident_notional_cost = parse_number(incident_notional_cost))

# incident_notional_cost from dataframe `animal_rescue` is now 'double' or numeric
typeof(animal_rescue$incident_notional_cost)

```

Now that incident_notional_cost is numeric, let us quickly calculate summary statistics for each animal group. 


```{r, stats_on_incident_cost,message=FALSE, warning=FALSE}

animal_rescue %>% 
  
  # group by animal_group_parent
  group_by(animal_group_parent) %>% 
  
  # filter resulting data, so each group has at least 6 observations
  filter(n()>6) %>% 
  
  # summarise() will collapse all values into 3 values: the mean, median, and count  
  # we use na.rm=TRUE to make sure we remove any NAs, or cases where we do not have the incident cos
  summarise(mean_incident_cost = mean (incident_notional_cost, na.rm=TRUE),
            median_incident_cost = median (incident_notional_cost, na.rm=TRUE),
            sd_incident_cost = sd (incident_notional_cost, na.rm=TRUE),
            min_incident_cost = min (incident_notional_cost, na.rm=TRUE),
            max_incident_cost = max (incident_notional_cost, na.rm=TRUE),
            count = n()) %>% 
  
  # sort the resulting data in descending order. You choose whether to sort by count or mean cost.
  arrange(desc(mean_incident_cost))

```

### Compare the mean and the median for each animal group. waht do you think this is telling us?

The median is typically lower than the mean for most of the animal groups. This suggests that there are some large positive outliers that are potentially disproportionately increasing the mean and introducing bias. 


Finally, let us plot a few plots that show the distribution of incident_cost for each animal group.

```{r, plots_on_incident_cost_by_animal_group,message=FALSE, warning=FALSE}

# base_plot
base_plot <- animal_rescue %>% 
  group_by(animal_group_parent) %>% 
  filter(n()>6) %>% 
  ggplot(aes(x=incident_notional_cost))+
  facet_wrap(~animal_group_parent, scales = "free")+
  theme_bw()

base_plot + geom_histogram()
base_plot + geom_density()
base_plot + geom_boxplot()
base_plot + stat_ecdf(geom = "step", pad = FALSE) +
  scale_y_continuous(labels = scales::percent)



```

### Which of these four graphs do you think best communicates the variability of the `incident_notional_cost` values? 

...also, can you please tell some sort of story (which animals are more expensive to rescue than others, the spread of values) and speculate about the differences in the patterns.

I believe that the boxplot is the best way to visualise the variability in the values. This is because it clearly highlights the median, quartiles and also the presence of outliers (which really help to show large spread in the data). By showing the quartiles, we can clearly see how narrow the distribution of the bulk of the values is. The quartiles are also useful in showing us potential symmetry within the distribution. 

Typically, we expect the categories with fewer data points (smaller counts) to exhibit greater range - because outlying data points will have a much larger effect on the location of the quartiles than in data sets with many values. 




# Submit the assignment

Knit the completed R Markdown file as an HTML document (use the "Knit" button at the top of the script editor window) and upload it to Canvas.

## Details

If you want to, please answer the following

-   Who did you collaborate with: Nobody.
-   Approximately how much time did you spend on this problem set: A couple of days.
